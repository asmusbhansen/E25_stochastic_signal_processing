{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d08f2957-6ae9-4f6b-ba8c-e2b8ca0fd11d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Lecture Notes\n",
    "\n",
    "#### Estimators\n",
    "\n",
    "Estimator - Rule/Function that estimates \\\\(\\theta\\\\) from a given realization of data\n",
    "\n",
    "$$\\hat{\\theta}=g(x[0],..,x[N-1])$$\n",
    "\n",
    "To design an estimator, we need both a signal model, which links our observed data to our parameters of interest, and an objective function. Then finding an estimator, becomes an optimization problem of minimizing the objective function.\n",
    "\n",
    "![estimator_design.png](./estimator_design.png \"estimator_design.png\")\n",
    "\n",
    "Unbiased estimator - On average, we expect the estimator estimate to be equal to the true value \\\\(\\mathbb{E}[\\hat{\\theta}]=\\theta\\\\)\n",
    "\n",
    "Minimum variance - We aim to use an estimator with minimum variance, since a lower variance will give us a estimates closer to the true value\n",
    "\n",
    "Information in data - The joint PDF \\\\(p(\\mathbf{x};\\theta)\\\\) (Reads the probavility of x given parameters \\\\(\\theta\\\\)) will show how much information is present in observed data. The more the PDF depends on the uknown parameter \\\\(\\theta\\\\). the more accurate an estimator we will have.\n",
    "\n",
    "If \\\\(x[0]=A+w[0]\\\\) where \\\\(w\\\\) is normally distributed with mean 0 and \\\\(\\sigma_w^2\\\\) and we want to estimate A, the joint PDF are as seen below\n",
    "\n",
    "![pdf.png](./pdf.png \"pdf.png\")\n",
    "\n",
    "In the first example, the PDF is \"sharper\" and will give a better estimate.\n",
    "\n",
    "Likelihood function - The PDF of data(Where data is fixed) and the unknown parameter \\\\(\\theta\\\\) varies is called the likelihood function. The sharpness of this functions determines hoc much information about \\\\(\\theta\\\\) i present in data and how acccurately we can estimate it.\n",
    "\n",
    "The sharpness is quantified by the curvature of the log-likelihood function\n",
    "\n",
    "$$I(\\theta) = -E_{\\mathbf{x}}\\left[\\frac{\\partial^2 \\ln p[\\mathbf{x}; \\theta]}{\\partial \\theta^2}\\right]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba66b19e-6348-42aa-a624-3a35b2bb9ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Maximum Likelihood estimator\n",
    "\n",
    "When given a set of observations \\\\(\\mathbf{x}\\\\) the probability of observing this data , given a parameter \\\\(\\theta\\\\) is given by \\\\(p(\\mathbf{x};\\theta)\\\\)\n",
    "\n",
    "The goal for MLE is the maximize the likelihood given the data \n",
    "\n",
    "$$\\hat{\\theta} = \\arg \\max_{\\theta} p(\\mathbf{x}; \\theta)$$\n",
    "\n",
    "For convenience in optimization, we often use the log-likelihood function $\\ell(\\theta)$, defined as\n",
    "\n",
    "$$\\ell(\\theta) = \\log p(\\mathbf{x}; \\theta) = \\sum_{i=1}^{n} \\log p(x_i; \\theta)$$\n",
    "\n",
    "MLE estimates are asymptotically unbiased, i.e.,$$\\lim_{N \\to \\infty} E[\\hat{\\theta}] = \\theta.$$MLE estimates are asymptotically efficient$$\\lim_{N \\to \\infty} \\text{VAR}[\\hat{\\theta}] \\to I^{-1}(\\theta).$$\n",
    "\n",
    "Where \\\\(I^{-1}(\\theta)\\\\) is the inverse of the Fischer information matrix shown above, called the Cramer-Rao lower bound which defines the absolute minimum variance that any unbiased estimator can acheive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26b5bead-382e-403f-9242-7dfde599c410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Least Squares Estimator\n",
    "\n",
    "If we have a signal model where the signal \\\\(\\theta\\\\) is corrupted by the noise, measurements or model inaccuracies and we measure \\\\(s\\\\)\n",
    "\n",
    "$$\\mathbf{s}=\\mathbf{H}\\mathbf{\\theta}$$\n",
    "\n",
    "Here \\\\(\\mathbf{H}\\\\) is the observation matrix. Then\n",
    "\n",
    "The cost function \\\\(J(\\theta)\\\\) is defined as:\n",
    "$$J(\\theta) = \\sum_{n=0}^{N-1} (x[n] - s[n])^2$$\n",
    "\n",
    "$$= (\\mathbf{x} - \\mathbf{H}\\boldsymbol{\\theta})^T (\\mathbf{x} - \\mathbf{H}\\boldsymbol{\\theta})$$\n",
    "\n",
    "The value of \\\\(\\mathbf{\\theta}\\\\) that minimizes the above cost function is the LSE (Least Squares Estimator):\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = (\\mathbf{H}^T \\mathbf{H})^{-1} \\mathbf{H}^T \\mathbf{y}$$\n",
    "\n",
    "With the least squares estimator we don't use any probabalistic models, but we define a mathematical model of our signal, based on some parameters \\\\(\\theta\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9af2273-5ac3-444b-a401-fd2e72a91608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercises\n",
    "\n",
    "![ex_11_a.png](./ex_11_a.png \"ex_11_a.png\")\n",
    "![ex_11_b.png](./ex_11_b.png \"ex_11_b.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7997794e-ca9f-4a1b-8495-59fabe429cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1)\n",
    "\n",
    "The LS estimator in closed form is\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = (\\mathbf{H}^T \\mathbf{H})^{-1} \\mathbf{H}^T \\mathbf{y}$$\n",
    "\n",
    "Where \\\\(\\mathbf{H}\\\\) is the design matrix with rows \\\\(\\mathbf{h_1}=[1, x_1, x_1^2]\\\\) to \\\\(\\mathbf{h_N}=[1, x_N, x_N^2]\\\\) for \\\\(M=3\\\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba35879a-c2fe-4668-9d7a-d285ee310635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de738891-56cc-49d4-8d84-f6315ea0ef19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 100\n",
    "\n",
    "i = np.arange(1,N+1)\n",
    "\n",
    "x = np.sin(2*np.pi*i/N)\n",
    "\n",
    "w = np.random.normal(0, 0.2, N)\n",
    "\n",
    "y = x + w\n",
    "\n",
    "train_samples = 10\n",
    "M = 3\n",
    "\n",
    "train_idx = np.random.randint(low=0, high=N, size=train_samples)\n",
    "\n",
    "y_train = y[train_idx]\n",
    "x_train = x[train_idx]\n",
    "\n",
    "H = np.zeros((N, M+1))\n",
    "\n",
    "for t in range(N):\n",
    "    for m in range(M+1):\n",
    "        H[t, m] = i[t]**m\n",
    "\n",
    "H_train = H[train_idx, :]\n",
    "\n",
    "theta_hat = np.linalg.pinv(H_train) @ x_train\n",
    "\n",
    "x_hat = H @ theta_hat\n",
    "\n",
    "print(theta_hat)\n",
    "plt.figure()\n",
    "plt.plot(x, label=\"x\")\n",
    "plt.plot(y, label=\"y\")\n",
    "plt.plot(x_hat, label=\"x_hat\")\n",
    "plt.plot(x-x_hat, label=\"Error\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1957888f-7699-4335-9b21-e8c5f5e37a25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "![ex_11_c.png](./ex_11_c.png \"ex_11_c.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62758ff3-4a1a-43a2-867f-5e9a8eb1a01c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### 1 and 2)\n",
    "\n",
    "The likelihood function \n",
    "\n",
    "$$\n",
    "p(\\mathbf{x};A) = \\prod_{n=0}^{N-1} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\,\\exp\\left(-\\frac{(x_n-A)^{2}}{2\\sigma^{2}}\\right)\n",
    "$$\n",
    "\n",
    "Then the log likelihood function is\n",
    "\n",
    "$$\n",
    "\\ell(\\mathbf{x};A)\n",
    "= \\sum_{n=0}^{N-1} \\log\\ \\left(\n",
    "\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\n",
    "\\exp \\left[-\\frac{(x_n-A)^2}{2\\sigma^{2}}\\right]\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ell(\\mathbf{x};A)= -\\frac{N}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{n=0}^{N-1}(x_n - A)^2\n",
    "$$\n",
    "\n",
    "It can be seen that to maximize this, we will need to minimize \\\\(\\sum_{n=0}^{N-1}(x_n - A)^2\\\\) which is the least squares problem."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lecture11",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
