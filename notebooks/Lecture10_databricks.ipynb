{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c6e2fff-e45c-4e67-81b6-f0f01d9eb129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lecture Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04b1d1f9-ca33-4328-8bf4-31199317b0ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We want to estimate the random variable X from a noisy oberserved variable Y\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}}=A\\mathbf{y}+\\mathbf{b}\n",
    "$$\n",
    "\n",
    "Such that the error $\\mathbb{E}[||\\mathbf{x}-\\mathbf{\\hat{x}}||]$ is minimized\n",
    "\n",
    "By defining the zero mean vectors $\\mathbf{\\tilde{x}}=x-\\mu_x$ and $\\mathbf{\\tilde{y}}=y-\\mu_y$ we rewrite\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}}=A(\\mathbf{\\tilde{y}}+\\mu_y)+\\mathbf{b}=A\\mathbf{\\tilde{y}}+(A\\mu_y+\\mathbf{b})\n",
    "$$\n",
    "\n",
    "To preserve the mean we set $A\\mu_y+\\mathbf{b}=\\mu_x$\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}}=A\\mathbf{\\tilde{y}}+\\mu_x\n",
    "$$\n",
    "\n",
    "And we find the optimal $\\mathbf{b}=\\mu_x-A\\mu_y$\n",
    "\n",
    "Then we can work only on the $A$ term when optimizing.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[||\\mathbf{\\tilde{x}} - A\\mathbf{\\tilde{y}}||]=\\mathbf{E}[(\\mathbf{\\tilde{x}} - A\\mathbf{\\tilde{y}})^T(\\mathbf{\\tilde{x}} - A\\mathbf{\\tilde{y}})]\n",
    "$$\n",
    "\n",
    "This we can differentiate with with respect to $A$ and set it to zero which gives us\n",
    "\n",
    "$$\n",
    "A=C_{xy}C_{yy}^{-1}\n",
    "$$\n",
    "\n",
    "Where $C_{xy}$ and $C_{yy}$ are the cross and auto correlation matrices respectively.\n",
    "\n",
    "This works for any joint distribution of $X$ and $Y$ and only requires the first two moments of the distributions, mean and covariance.\n",
    "\n",
    "$$\n",
    "\\mathbf{\\tilde{x}}=\\mu_x+C_{xy}C_{yy}^{-1}(y-\\mu_y)\n",
    "$$\n",
    "\n",
    "### Special case for additive Gaussian noise(Wiener filter)\n",
    "\n",
    "If we assume the model $\\mathbb{y}=H\\mathbb{x}+n$ where $n$ is zero mean noise independent of x with covariance $C_{nn}$\n",
    "\n",
    "Then by linear transformation we can find the cross covariance defined by the linear transformation \n",
    "$$\n",
    "C_{xy}=C_{xx}H^T\n",
    "$$\n",
    "\n",
    "And the autocovariance(See lecture slides for proofs)\n",
    "\n",
    "$$\n",
    "C_{yy}=HC_{xx}H^T+C_{nn}\n",
    "$$\n",
    "\n",
    "Since we perform a linear transformation\n",
    "\n",
    "We can plug this into the original solution\n",
    "\n",
    "$$\n",
    "\\mathbf{\\tilde{x}}=\\mu_x+C_{xx}H^T(HC_{xx}H^T+C_{nn})^{-1}(y-\\mu_y)\n",
    "$$\n",
    "\n",
    "### MSE For linear Minimum Mean Square Estimators\n",
    "\n",
    "The MSE is defined as \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(x-\\tilde{x})^2]\n",
    "$$\n",
    "\n",
    "After minimizing the MSE and solving for the estimation parameters, the theoretical minimum MSE is obtained by\n",
    "\n",
    "$$\n",
    "MSE(\\tilde{x})=C_{xx}-C_{xy}C_{xx}^{-1}C_{xy}\n",
    "$$\n",
    "\n",
    "For the scalar case, this reduces to\n",
    "\n",
    "$$\n",
    "MSE(\\tilde{x})=Var(x)-\\frac{Cov(x,y)^2}{Var(y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "960a5f43-0c03-45e7-803c-648f1e54a74f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82638802-9d72-40f1-8d9b-04ea6c7fa379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![image.png](attachment:2fc970f2-8591-4378-bc36-e7896f02547f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aa002ff-4366-484d-93c3-f377e9e57de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ar_one(w, theta):\n",
    "\n",
    "    N = len(w)\n",
    "\n",
    "    x = np.zeros_like(w)\n",
    "    \n",
    "    x[0] = w[0]\n",
    "    for i in range(1, N):\n",
    "        x[i] = theta*x[i-1]+w[i]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a7dd120-2b85-44b0-8f5b-72b9b1b3cb1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# a and b)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "N = 1000\n",
    "nlags = 20\n",
    "sigma_w = 1\n",
    "\n",
    "w_ = []\n",
    "x_ = []\n",
    "\n",
    "r_ = [0.1, 0.5, 0.9]\n",
    "for r in r_:\n",
    "    w = np.random.normal(0, 1, N)\n",
    "\n",
    "    x = ar_one(w, r)\n",
    "\n",
    "    w_ += [w]\n",
    "    x_ += [x]\n",
    "    \n",
    "    acorr = acf(x, nlags=nlags)\n",
    "    r_0 = sigma_w**2/(1-r**2)\n",
    "    acorr_theoretical = [r_0*r**np.abs(k) for k in range(nlags+1)]\n",
    "    acorr_theoretical = [a/acorr_theoretical[0] for a in acorr_theoretical]\n",
    "\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(w, label=\"Noise\")\n",
    "    plt.plot(x, label=\"AR(1)\")\n",
    "    plt.title(f\"AR(1) = {r}\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(acorr, label=\"Empirical\")\n",
    "    plt.plot(acorr_theoretical, label=\"Theoretical\")\n",
    "    plt.title(f\"Autocorrelation r={r}\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6baf1250-6a68-4b14-8b56-6d7b88843c6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We want to estimate some signal $x$ from several observations $\\mathbf{y_n}$\n",
    "\n",
    "$$\n",
    "\\hat{x_n}=\\mathbf{h}^T\\mathbf{y_n}\n",
    "$$\n",
    "\n",
    "From LMMSE we know that \n",
    "\n",
    "$$\n",
    "\\hat{x}_n=C_{xy}C_{yy}^{-1}\\mathbf{y}\n",
    "$$\n",
    "\n",
    "Where the covariance matrices becomes correlation matrices\n",
    "\n",
    "$$\n",
    "\\mathbf{h}=R_{yy}^{-1}\\mathbf{r}_{xy}\n",
    "$$\n",
    "\n",
    "Because we estimate a scalar from a vector, we take the first row of the cross covariance matrix instead of the whole matrix.\n",
    "\n",
    "We can estimate both present(denoising), future(prediction) and previous(smoothing) values using the Wiener filter. This comes down to how the cross correlation vector is defined\n",
    "\n",
    "$\\tau=0$ is denoising, $\\tau>0$ is prediction and $\\tau < 0$ is smoothing\n",
    "\n",
    "### Filtering\n",
    "\n",
    "For filtering, we use the following model\n",
    "\n",
    "$$\n",
    "y_n=x_n+w_n\n",
    "$$\n",
    "\n",
    "Where $x_n$ is the signal and $w_n$ is noise uncorrelated to the signal.\n",
    "\n",
    "We know that $C_{xx}=R_{ss}+R_{ww}$ since signal and noise is uncorrelated.\n",
    "The observations are formed as the vector $\\mathbf{y}_n=[y[0],..,y[n]]$\n",
    "\n",
    "We then define the covariance matrix \n",
    "\n",
    "$$\n",
    "C_{xy}=\\mathbb{E}[x_n\\mathbf{y}_n]=\\mathbb{E}[x_n(\\mathbf{x}_n+\\mathbf{w}_n)]=\\mathbb{E}[x_n\\mathbf{x}_n]+\\mathbb{E}[x_n\\mathbf{w}_n]=\\mathbb{E}[x_n\\mathbf{x}_n]=[r_{xx}(n),..,r_{xx}(0)]=\\mathbf{r}_{xx}' \n",
    "$$\n",
    "\n",
    "The small \"'\" sign means that the vector is flipped. That is fine by us, since Python flips the second array when convolving.\n",
    "\n",
    "Since $w_n$ and $x_n$ are uncorrelated $\\mathbb{E}[s_n\\mathbf{x}_n]$=0\n",
    "\n",
    "This gives us the Wiener-Hopf equations\n",
    "\n",
    "\\begin{equation}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "r_{yy}[0]   & r_{yy}[-1] & \\cdots & r_{yy}[1-N]\\\\[4pt]\n",
    "r_{yy}[1]   & r_{yy}[0]  & \\cdots & r_{yy}[2-N]\\\\[4pt]\n",
    "\\vdots      & \\vdots     & \\ddots & \\vdots      \\\\[4pt]\n",
    "r_{yy}[N-1] & r_{yy}[N-2]& \\cdots & r_{yy}[0]\n",
    "\\end{bmatrix}}_{\\mathbf r_{yy}}\n",
    "\\;\n",
    "\\underbrace{\\begin{bmatrix} h_0\\\\ h_1\\\\ \\vdots\\\\ h_{N-1}\\end{bmatrix}}_{\\mathbf h}\n",
    "\\;=\\;\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "r_{xx}[0]\\\\[4pt]\n",
    "r_{xx}[1]\\\\[4pt]\n",
    "\\vdots\\\\[4pt]\n",
    "r_{xx}[N-1]\n",
    "\\end{bmatrix}}_{\\mathbf r_{xx}}.\n",
    "\\end{equation}\n",
    "\n",
    "We can calculate $r_{xx}$ from the estimated $r_{yy}$. From the signal model $y_n=x_n+w_n$ we calculate the covariance to be $Cov_y(k)=Cov_x(k)$ for $k \\neq 0$ and $Cov_y(0)=Cov_x(0)+\\sigma_w^2$ for $k=0$. Hence $r_{xx}(k)=r_{yy}(k)$ for $k \\neq 0$ and $r_{xx}(0)=r_{yy}(0)-\\sigma_w^2$\n",
    "\n",
    "### Prediction\n",
    "\n",
    "We want to estimate $x=y_{N-1+I}$ from the last $[y[0],..,y[N-1]]$ observations\n",
    "\n",
    "The covariance matrix becomes\n",
    "\n",
    "$$\n",
    "C_{xy}=\\mathbb{E}[x_n \\mathbf{y}_n]=\\mathbb{E}[y_{N-1+I} \\mathbf{y}_n]=\\mathbb{E}[y_{N-1+I} [y_0,..y_{N-1}]]=[r_{yy}(N-1+I),..r_{yy}(I)]=\\mathbf{r}_{yy}'\n",
    "$$\n",
    "\n",
    "Now the Wiener-Hopf equations are \n",
    "\n",
    "\\begin{equation}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "r_{yy}[0]   & r_{yy}[-1] & \\cdots & r_{yy}[1-N]\\\\[4pt]\n",
    "r_{yy}[1]   & r_{yy}[0]  & \\cdots & r_{yy}[2-N]\\\\[4pt]\n",
    "\\vdots      & \\vdots     & \\ddots & \\vdots      \\\\[4pt]\n",
    "r_{yy}[N-1] & r_{yy}[N-2]& \\cdots & r_{yy}[0]\n",
    "\\end{bmatrix}}_{\\mathbf r_{yy}}\n",
    "\\;\n",
    "\\underbrace{\\begin{bmatrix} h_0\\\\ h_1\\\\ \\vdots\\\\ h_{N-1}\\end{bmatrix}}_{\\mathbf h}\n",
    "\\;=\\;\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "r_{yy}[I]\\\\[4pt]\n",
    "r_{yy}[I+1]\\\\[4pt]\n",
    "\\vdots\\\\[4pt]\n",
    "r_{yy}[I+N-1]\n",
    "\\end{bmatrix}}_{\\text{RHS prediction vector}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e795ea-53c0-4b3d-953c-7e73db15c968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# c, d, e and f)\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "P = 3\n",
    "tau = 1\n",
    "\n",
    "r_ = [0.1, 0.5, 0.9]\n",
    "for r in r_:\n",
    "    w = np.random.normal(0, 1, N)\n",
    "\n",
    "    x = ar_one(w, r)\n",
    "\n",
    "    acf_values = acf(x, nlags=P+tau)\n",
    "    \n",
    "    cov = np.zeros((P,P))\n",
    "    cross = np.zeros(P)\n",
    "    for i in range(P):\n",
    "        cross[i] = acf_values[i+tau]\n",
    "        for j in range(P):\n",
    "            cov[i, j] = acf_values[np.abs(i-j)]\n",
    "\n",
    "    h = np.linalg.inv(cov) @ cross\n",
    "\n",
    "    x_hat = np.convolve(x, h, mode='full')[:-(P-1)]\n",
    "\n",
    "    e = x-x_hat\n",
    "    mse = np.mean(e**2)\n",
    "\n",
    "    plot_num = 100\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(x[:plot_num], label=\"x\")\n",
    "    plt.plot(x_hat[:plot_num], label=\"x one-step-predicted\")\n",
    "    #plt.plot(e, label=\"error\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"r={r}, MSE={mse:.3f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5fcf297-9c57-42ae-b7d9-59e7a3d3f18f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As r grows larger, a better prediction performance is expected, since the process is less random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daa9ddd1-35e6-4fff-9858-1c148e5bf6b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![image.png](attachment:65a5eba3-a5b9-4847-afc9-376946d18c36.png)\n",
    "![image.png](attachment:125da7ae-ae1e-40a8-9d02-f3b2ca9b849a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b5e192-8c47-4076-a450-567657d50817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "N = 1000\n",
    "nlags = 20\n",
    "sigma_w = 1\n",
    "sigma_e = 1\n",
    "w_ = []\n",
    "x_ = []\n",
    "\n",
    "r_ = [0.1, 0.5, 0.9]\n",
    "for r in r_:\n",
    "    w = np.random.normal(0, 1, N)\n",
    "    e = np.random.normal(0, 1, N)\n",
    "    x = ar_one(w, r)\n",
    "\n",
    "    z = x + e\n",
    "\n",
    "    w_ += [w]\n",
    "    x_ += [x]\n",
    "    \n",
    "    acorr = acf(z, nlags=nlags)\n",
    "\n",
    "    r_0 = sigma_w**2/(1-r**2)\n",
    "    acorr_theoretical = [(r_0+sigma_e**2) if k==0 else r_0*r**np.abs(k) for k in range(nlags+1)]\n",
    "    acorr_theoretical = [a/acorr_theoretical[0] for a in acorr_theoretical]\n",
    "\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(w, label=\"Noise\")\n",
    "    plt.plot(x, label=\"AR(1)\")\n",
    "    plt.title(f\"AR(1) = {r}\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(acorr, label=\"Empirical\")\n",
    "    plt.plot(acorr_theoretical, label=\"Theoretical\")\n",
    "    plt.title(f\"Autocorrelation r={r}\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "037c7491-5b97-4d73-9e01-687be1ee3165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2, 3, 4 and 5)\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "N = 1000\n",
    "P = 3\n",
    "tau = 0\n",
    "sigma_w = 1\n",
    "r_ = [0.1, 0.5, 0.9]\n",
    "var_e_ = [0.25, 1.0, 9.0]\n",
    "\n",
    "for var_e in var_e_:    \n",
    "\n",
    "    for r in r_:\n",
    "        w = np.random.normal(0, 1, N)\n",
    "        e = np.random.normal(0, np.sqrt(var_e), N)\n",
    "        z = ar_one(w, r)\n",
    "        x = z + e\n",
    "        r_0 = sigma_w**2/(1-r**2)\n",
    "        acorr = acf(x, nlags=20, fft=True) * np.var(x)\n",
    "        \n",
    "        # We compute the cross an correlations analytically\n",
    "        rxx = np.zeros((P,P))\n",
    "        rzz = np.zeros(P)\n",
    "        for i in range(P):\n",
    "            \n",
    "            # For the cross correlation, the noise term cancels out\n",
    "            rzz[i] = acorr[i+tau]\n",
    "            if i+tau == 0:\n",
    "                rzz[i+tau] -= var_e\n",
    "            \n",
    "            for j in range(P):\n",
    "                rxx[i, j] = acorr[np.abs(i-j)]\n",
    "    \n",
    "        h = np.linalg.solve(rxx, rzz)\n",
    "        z_hat = np.convolve(x, h, mode='full')[:-(P-1)]\n",
    "        print(f\"r={r}, sigma_e^2={var_e} - Filter coeffecients: {h[::-1]}\")\n",
    "        e = z-z_hat\n",
    "        mse = np.mean(e**2)\n",
    "\n",
    "        # Using vector version\n",
    "        mse_theoretical = rzz[0] - rzz.T @ np.linalg.inv(rxx) @ rzz\n",
    "        \n",
    "        plot_num = 100\n",
    "\n",
    "        title = f\"r={r:.2f}, var_e={var_e:.2f}, MSE={mse:.3f}, MSE Expected={mse_theoretical:.3f}\"\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(z[:plot_num], label=\"z\")\n",
    "        plt.plot(x[:plot_num], label=\"x\", alpha=0.3)\n",
    "        plt.plot(z_hat[:plot_num], label=\"z denoised(From x)\")\n",
    "        plt.legend()\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e2c6a3b-fad6-4eee-9840-9c1a4a130d53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lecture10_databricks",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
