{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f034521b-b3fa-4332-8e1e-0e7a2723f8e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Get current notebook directory\n",
    "notebook_dir = os.getcwd()\n",
    "file_1_path = os.path.join(notebook_dir, \"input1.mat\")\n",
    "file_2_path = os.path.join(notebook_dir, \"input2.mat\")\n",
    "file_3_path = os.path.join(notebook_dir, \"input3.mat\")\n",
    "\n",
    "input_1 = loadmat(file_1_path)[\"X\"].flatten()\n",
    "input_2 = loadmat(file_2_path)[\"X\"].flatten()\n",
    "input_3 = loadmat(file_3_path)[\"X\"].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c96c5d-7f8a-4418-a7c7-23ee1d5e393a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "###a) \n",
    "As a starting point, analyze the time series data sets and choose appropriate parameters e.g., filter order which is the same as the number of present and past input samples to predict the series. Based on your analysis of these time series, which filter order would you choose for the prediction of these series? Briefly explain the reasons behind your choices. (Hint: using ACF and partial ACF measures to guide your analysis) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70a2ea67-a82d-45d7-8a03-9139801bf84a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for n, data in enumerate([input_1, input_2, input_3]):\n",
    "\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(10, 8))\n",
    "\n",
    "    ax[0].plot(data)\n",
    "    ax[0].set_title(f\"Input {n+1}: Raw data Mean = {np.mean(data):.2f}\")\n",
    "\n",
    "    # Plot ACF\n",
    "    plot_acf(data, ax=ax[1], lags=20, title=f\"Input {n+1}: Autocorrelation Function\")\n",
    "\n",
    "    # Plot PACF\n",
    "    plot_pacf(data, ax=ax[2], lags=20, title=f\"Input {n+1}: Partial Autocorrelation Function\", method=\"ywm\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31fa24f5-9af6-4687-a2dc-0ea9cc7eded2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "All inputs have a mean close to 0.\n",
    "\n",
    "####Input 1\n",
    "\n",
    "We see a decaying ACF and one significant peak in the PACF, so a filter order of one is chose for prediction\n",
    "This implies that the data can be modelled using an AR(1) model.\n",
    "\n",
    "####Input 2\n",
    "\n",
    "We see a decaying ACF and one significant peak in the PACF, so a filter order of one is chose for prediction\n",
    "This implies that the data can be modelled using an AR(1) model.\n",
    "\n",
    "####Input 1\n",
    "\n",
    "We see a decaying ACF and two significant peaks in the PACF, so a filter order of two is chose for prediction\n",
    "This implies that the data can be modelled using an AR(2) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b49ef92-6c8f-4fb8-b862-0e1f0f296980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###b) \n",
    "\n",
    "By choosing to predict these time series from previous samples, we are implicitly assuming that the time series exhibit an autoregressive nature. From your analysis of the time series in the previous part, can you confirm whether this assumption is true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61c1c631-20aa-4aa9-8902-a4bbef9f4c2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It can be assumed that the series have an autoregressive nature. See response above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c223847-c59b-4370-8c22-0a8031d5b377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###c) \n",
    "Design 1-step ahead Wiener filter for the given time series. Using the filter coefficients, predict the time series (1-step ahead) and plot the predicted signals along with the true signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95a49d76-5540-4dd8-aab4-a801f7a5ae50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From the theory on Wiener filters, we know that an optimal filter for linear prediction can be found as\n",
    "\n",
    "$$\n",
    "\\mathbf{h}=R_{yy}^{-1}\\mathbf{r}_{xy}\n",
    "$$\n",
    "\n",
    "Where \\\\(y\\\\) is the observed signal, and \\\\(x\\\\) is the parameter to estimate.\n",
    "\n",
    "In prediction, the cross covariance can be expressed as \n",
    "\n",
    "$$\n",
    "C_{xy}=\\mathbb{E}[x_n \\mathbf{y}_n]=\\mathbb{E}[y_{N-1+I} \\mathbf{y}_n]=\\mathbb{E}[y_{N-1+I} [y_0,..y_{N-1}]]=[r_{yy}(N-1+I),..r_{yy}(I)]=\\mathbf{r}_{yy}'\n",
    "$$\n",
    "And since we have one step prediction, we set \\\\(I=1\\\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "969d27b1-9ab6-4215-a51c-069ba0da621c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "P_ = [1, 1, 2]\n",
    "I = 1\n",
    "for n, x in enumerate([input_1, input_2, input_3]):\n",
    "    P = P_[n]\n",
    "    var_x = np.var(x)\n",
    "    print(f\"Input {n+1} - Filter length={P}, I={I}, Variance: {var_x}\")\n",
    "\n",
    "    acf_values = acf(x, nlags=P+I, fft=False) * var_x\n",
    "    \n",
    "    rxx = np.zeros((P,P))\n",
    "    rxy = np.zeros(P)\n",
    "    for i in range(P):\n",
    "        rxy[i] = acf_values[i+I]\n",
    "        for j in range(P):\n",
    "            rxx[i, j] = acf_values[np.abs(i-j)]\n",
    "\n",
    "    h = np.linalg.solve(rxx, rxy)\n",
    "    print(f\"Filter coeffecients: {h}\")\n",
    "\n",
    "    x_hat = np.convolve(x, h[::-1], mode='same')#[:len(x)]\n",
    "    # Shift predcitions by one to line up with true value\n",
    "    x_hat = np.concatenate([[0], x_hat])[:-1]\n",
    "\n",
    "    # We ignore the first P samples where our estimate is invalid\n",
    "    mse = np.mean((x[P:] - x_hat[P:])**2)\n",
    "    rmse = mse / var_x\n",
    "\n",
    "    plot_num = 100\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(x[:plot_num], label=f\"Input {n+1}\")\n",
    "    plt.plot(x_hat[:plot_num], label=f\"Input {n+1} one-step-predicted\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Input {n+1} - MSE={mse:.3f}, rMSE={rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcdc2186-193d-4042-afac-cffc6b315d1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###d) \n",
    "Observe and comment on the accuracy of the predictors. To obtain a quantitative measure of accuracy, compute and compare the mean squared error of the resulting prediction filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34eb6119-b044-49cb-b666-40a3b448839e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "To better interpret the MSE values, I have calculated the relative MSE as rMSE = MSE / Var(signal)\n",
    "\n",
    "I also notice that for one step prediction, I have a MSE close to 1. This could indicate that the processes are generated using noise with variance 1, since when we perform one step prediction(On an AR(p) process), the expected error is the noise variance of the process.\n",
    "\n",
    "#### Input 1\n",
    "We have a high relative MSE which is expected since the low PACF values and filter coefficients imply a low correlation\n",
    "\n",
    "#### Input 2\n",
    "We have a low relative MSE which again is expected because of the high PACF values. However, since lag 1 have a high correlation with the current sample, the filter coefficient is large.\n",
    "\n",
    "#### Input 3\n",
    "\n",
    "Here we have a medium(Compared to the other two) relative MSE, again implied by the PACF values(Approx 0.75-0.5). The filter coeffecients acts a a low pass filter, weighing the lag 1 sample higher, and predicts the current value as the weighted mean of the two last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84aa6613-84c1-47fb-9bb1-5a78892b3ad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###e) \n",
    "Observe the values of filter coefficients that you obtained. Do these coefficients give any information about the nature of the underlying time series (Hint: Note that you implicitly assumed the autoregressive (AR) nature of the time series.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a43474e0-4cef-4a35-8c8f-4c9b488ac89e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "See answer above"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
